{
  "cells": [
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2024-12-03T11:07:57.318940Z",
          "start_time": "2024-12-03T11:07:57.313420Z"
        },
        "id": "initial_id"
      },
      "source": [
        "import re"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T11:29:15.295191Z",
          "start_time": "2024-12-03T11:29:14.159771Z"
        },
        "id": "8f6169cecdc5faef",
        "outputId": "2794f9a6-0b4f-425b-9964-2516c79ba15b"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "input_file = \"posts_first_targil.xlsx\"\n",
        "\n",
        "# Read the Excel file with multiple sheets\n",
        "df = pd.read_excel(input_file, sheet_name=None)\n",
        "\n",
        "for sheet_name, data in df.items():\n",
        "    print(f\"Sheet name: {sheet_name} Headlines:, {list(data.columns)}\")"
      ],
      "id": "8f6169cecdc5faef",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sheet name: A-J Headlines:, ['sub_title', 'date', 'Newspaper', 'Body Text', 'title']\n",
            "Sheet name: BBC Headlines:, ['date', 'Newspaper', 'Body Text', 'title']\n",
            "Sheet name: J-P Headlines:, ['date', 'Newspaper', 'Body', 'title']\n",
            "Sheet name: NY-T Headlines:, ['date', 'Newspaper', 'Body Text', 'title']\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "b7b6eb676ad2fff4"
      },
      "cell_type": "markdown",
      "source": [],
      "id": "b7b6eb676ad2fff4"
    },
    {
      "metadata": {
        "id": "fb361094312c4d46"
      },
      "cell_type": "raw",
      "source": [
        "JP title update like the other files"
      ],
      "id": "fb361094312c4d46"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T11:29:18.002644Z",
          "start_time": "2024-12-03T11:29:17.989803Z"
        },
        "id": "64f4ca6cf3c34771",
        "outputId": "322b4407-6121-48cb-9e74-a517638b0d6b"
      },
      "cell_type": "code",
      "source": [
        "if \"J-P\" in df:\n",
        "    df[\"J-P\"].rename(columns={\"Body\": \"Body Text\"}, inplace=True)\n",
        "\n",
        "# Verify the change\n",
        "for sheet_name, data in df.items():\n",
        "    print(f\"Sheet name: {sheet_name} Headlines:, {list(data.columns)}\")"
      ],
      "id": "64f4ca6cf3c34771",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sheet name: A-J Headlines:, ['sub_title', 'date', 'Newspaper', 'Body Text', 'title']\n",
            "Sheet name: BBC Headlines:, ['date', 'Newspaper', 'Body Text', 'title']\n",
            "Sheet name: J-P Headlines:, ['date', 'Newspaper', 'Body Text', 'title']\n",
            "Sheet name: NY-T Headlines:, ['date', 'Newspaper', 'Body Text', 'title']\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "cfed82032f5b5b41"
      },
      "cell_type": "raw",
      "source": [],
      "id": "cfed82032f5b5b41"
    },
    {
      "metadata": {
        "id": "754ab94f6bb69555"
      },
      "cell_type": "markdown",
      "source": [
        "**Function to clean the data text**"
      ],
      "id": "754ab94f6bb69555"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T11:12:45.033741Z",
          "start_time": "2024-12-03T11:12:45.020935Z"
        },
        "id": "15e46b2d092cedee"
      },
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    regx = r\"((?<!\\w)[^\\s\\w]|[^\\s\\w](?!\\w))\"\n",
        "    dot_pattern = r\"(?<!\\w)([a-zA-Z]{2,})\\.([a-zA-Z]{2,})(?!\\w)\"\n",
        "    clean_t = re.sub(regx, r\" \\1 \", text)\n",
        "    clean_t = re.sub(dot_pattern, r\"\\1 . \\2\", clean_t)\n",
        "    return re.sub(r\"\\s+\", \" \", clean_t).strip()"
      ],
      "id": "15e46b2d092cedee",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "229fe1d446584279"
      },
      "cell_type": "raw",
      "source": [],
      "id": "229fe1d446584279"
    },
    {
      "metadata": {
        "id": "bbded466b605420a"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Part 2: Functions for processing data by lemmatize the text**"
      ],
      "id": "bbded466b605420a"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T11:12:50.268543Z",
          "start_time": "2024-12-03T11:12:49.420899Z"
        },
        "id": "a5bbc264e54c21ca"
      },
      "cell_type": "code",
      "source": [
        "# Load spaCy's language model\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "id": "a5bbc264e54c21ca",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T11:12:59.997500Z",
          "start_time": "2024-12-03T11:12:59.981336Z"
        },
        "id": "7bcd1ad78e7bc9f0"
      },
      "cell_type": "code",
      "source": [
        "def lemmatize_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    doc = nlp(text)\n",
        "    return \" \".join([token.lemma_ for token in doc])"
      ],
      "id": "7bcd1ad78e7bc9f0",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "c718ae6806f75031"
      },
      "cell_type": "raw",
      "source": [],
      "id": "c718ae6806f75031"
    },
    {
      "metadata": {
        "id": "412a36025083441f"
      },
      "cell_type": "markdown",
      "source": [
        "**Creating the processed files:**\n",
        "\n",
        "*A.* clean_file.xlsx\n",
        "\n",
        "*B.* lemma_file.xlsx\n",
        "\n",
        "**By each of the functions:**\n",
        "\n",
        "*A.* lemmatize_text()\n",
        "\n",
        "*B.* clean_text()"
      ],
      "id": "412a36025083441f"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T11:16:03.656612Z",
          "start_time": "2024-12-03T11:13:04.956031Z"
        },
        "id": "12517a4f8eb156b4",
        "outputId": "36c942f8-9160-4517-d4c6-81bd6000268c"
      },
      "cell_type": "code",
      "source": [
        "clean_sheets = {}\n",
        "lemma_sheets = {}\n",
        "for sheet_name, data in df.items():\n",
        "    # Apply clean_text to all string columns in the DataFrame\n",
        "    processed_clean_df = data.map(clean_text)\n",
        "    clean_sheets[sheet_name] = processed_clean_df\n",
        "    processed_lemma_df = data.map(lemmatize_text)\n",
        "    lemma_sheets[sheet_name] = processed_lemma_df\n",
        "\n",
        "# Save each processed sheet to a separate Excel file\n",
        "output_clean_file = \"output_files/clean_file.xlsx\"\n",
        "with pd.ExcelWriter(output_clean_file) as writer:\n",
        "    for sheet_name, processed_df in clean_sheets.items():\n",
        "        processed_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "output_lemma_file = \"output_files/lemma_file.xlsx\"\n",
        "with pd.ExcelWriter(output_lemma_file) as writer:\n",
        "    for sheet_name, processed_df in lemma_sheets.items():\n",
        "        processed_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "print(f\"Processed clean Excel file saved as: {output_clean_file}\")\n",
        "print(f\"Processed lemma Excel file saved as: {output_lemma_file}\")\n"
      ],
      "id": "12517a4f8eb156b4",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed clean Excel file saved as: output_files/clean_file.xlsx\n",
            "Processed lemma Excel file saved as: output_files/lemma_file.xlsx\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "fbe0a162d3051463"
      },
      "cell_type": "raw",
      "source": [],
      "id": "fbe0a162d3051463"
    },
    {
      "metadata": {
        "id": "dac0f8d4a549ae46"
      },
      "cell_type": "markdown",
      "source": [
        "***Part 3: using TF-IDF BM25/Okapi***"
      ],
      "id": "dac0f8d4a549ae46"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T11:29:23.364041Z",
          "start_time": "2024-12-03T11:29:23.350047Z"
        },
        "id": "abfbd881956b975c"
      },
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "from nltk.corpus import stopwords\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse import save_npz\n",
        "import nltk\n",
        "import os\n",
        "import pandas as pd\n",
        "\n"
      ],
      "id": "abfbd881956b975c",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T11:46:19.198624Z",
          "start_time": "2024-12-03T11:45:07.353534Z"
        },
        "id": "e48d50601fd82b5b",
        "outputId": "994dee41-4214-41db-defb-f3b0d5ac7c86"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize the set of English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define the function to filter out stopwords from text\n",
        "def filter_stopwords(text):\n",
        "    tokens = text.split()  # Split the input into tokens\n",
        "    return [token.lower() for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "# Process lemmatized documents and generate BM25 matrix\n",
        "input_folder = 'output_files'\n",
        "bm25_folder = 'bm25/clean'\n",
        "\n",
        "input_file = 'output_files/clean_file.xlsx'\n",
        "clean_df = pd.read_excel(input_file, sheet_name=None)\n",
        "for sheet_name, data in clean_df.items():\n",
        "    print(f\"Processing file: {sheet_name}\")\n",
        "\n",
        "    # Construct corpus by removing stopwords and combining text fields\n",
        "    if sheet_name == 'A-J':\n",
        "        documents = [\n",
        "            filter_stopwords(f'{record[\"title\"]} {record[\"sub_title\"]} {record[\"Body Text\"]}')\n",
        "            for _, record in data.iterrows()\n",
        "        ]\n",
        "    else:\n",
        "        documents = [\n",
        "            filter_stopwords(f'{record[\"title\"]} {record[\"Body Text\"]}')\n",
        "            for _, record in data.iterrows()\n",
        "        ]\n",
        "    # Create BM25 object\n",
        "    bm25_model = BM25Okapi(documents)\n",
        "\n",
        "    # Generate word-to-index mapping\n",
        "    words_in_vocab = bm25_model.idf.keys()\n",
        "    word_to_index = {word: idx for idx, word in enumerate(words_in_vocab)}\n",
        "\n",
        "    # Prepare data for sparse matrix\n",
        "    row_indices, col_indices, values = [], [], []\n",
        "    for word in words_in_vocab:\n",
        "        scores = bm25_model.get_scores(word)\n",
        "        for doc_index, score in enumerate(scores):\n",
        "            if score > 0:\n",
        "                row_indices.append(doc_index)\n",
        "                col_indices.append(word_to_index[word])\n",
        "                values.append(score)\n",
        "\n",
        "    # Build and save the sparse matrix\n",
        "    sparse_matrix = csr_matrix((values, (row_indices, col_indices)), shape=(len(documents), len(words_in_vocab)))\n",
        "    save_npz(os.path.join(bm25_folder, sheet_name.split('.')[0]), sparse_matrix)"
      ],
      "id": "e48d50601fd82b5b",
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing file: A-J\n",
            "Metadata stored in metadata/clean\\A-J\n",
            "Processing file: BBC\n",
            "Metadata stored in metadata/clean\\BBC\n",
            "Processing file: J-P\n",
            "Metadata stored in metadata/clean\\J-P\n",
            "Processing file: NY-T\n",
            "Metadata stored in metadata/clean\\NY-T\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T12:12:18.835402Z",
          "start_time": "2024-12-03T12:12:18.777105Z"
        },
        "id": "f7c8f9b416bcabc4",
        "outputId": "d3b70c33-8e02-442b-f736-4079ae5b41e7"
      },
      "cell_type": "code",
      "source": [
        "from scipy.sparse import load_npz\n",
        "\n",
        "# Load the sparse matrix\n",
        "sparse_matrix = load_npz(\"bm25/lemma/BBC.npz\")\n",
        "\n",
        "# Inspect the sparse matrix\n",
        "print(\"Sparse matrix shape:\", sparse_matrix.shape)\n",
        "print(\"Non-zero elements:\", sparse_matrix.nnz)\n",
        "print(\"Matrix contents:\")\n",
        "print(sparse_matrix.toarray())"
      ],
      "id": "f7c8f9b416bcabc4",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sparse matrix shape: (549, 13666)\n",
            "Non-zero elements: 1471283\n",
            "Matrix contents:\n",
            "[[0.         0.         0.         ... 2.94411559 0.         2.94411559]\n",
            " [0.         0.         0.         ... 2.90927687 0.         2.90927687]\n",
            " [0.         0.         0.         ... 2.94688451 0.         2.94688451]\n",
            " ...\n",
            " [0.         0.         0.         ... 2.48177516 0.         2.48177516]\n",
            " [0.         0.         0.         ... 2.64277891 0.         2.64277891]\n",
            " [0.         0.         0.         ... 2.77756136 0.         2.77756136]]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "b96a8d38d87ab409"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [],
      "id": "b96a8d38d87ab409"
    },
    {
      "metadata": {
        "id": "683891a07880b2f0"
      },
      "cell_type": "raw",
      "source": [],
      "id": "683891a07880b2f0"
    },
    {
      "metadata": {
        "id": "3a72524744be6885"
      },
      "cell_type": "markdown",
      "source": [
        "PART 3 - Word2Vec"
      ],
      "id": "3a72524744be6885"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T13:33:33.113419Z",
          "start_time": "2024-12-03T13:33:30.721134Z"
        },
        "id": "f65ee82a87e5ba4f",
        "outputId": "7bc658b2-37d1-48cb-ba13-4312889ab962"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import gensim.downloader as api\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.data import find\n",
        "import csv\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "id": "f65ee82a87e5ba4f",
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "de1d94cd4058a16e"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.data import find\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# File paths\n",
        "input_file = \"/content/output/lemma_file.xlsx\"  # Replace with your Excel file path\n",
        "output_file = \"/content/output/glove_lemma_withoutIdf_withoutStopWords.xlsx\"\n",
        "\n",
        "df = pd.read_excel(input_file, sheet_name=None)\n",
        "\n",
        "if \"J-P\" in df:\n",
        "    df[\"J-P\"].rename(columns={\"Body\": \"Body Text\"}, inplace=True)\n",
        "\n",
        "# Load GloVe vectors via gensim downloader\n",
        "try:\n",
        "    glove_model = api.load(\"glove-wiki-gigaword-300\")  # 300-dimensional GloVe vectors\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\d+\", \"\", text)  # Remove digits and dates\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
        "    tokens = word_tokenize(text)  # Tokenize the text\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    return [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
        "\n",
        "\n",
        "# Process each sheet\n",
        "results = []\n",
        "\n",
        "for sheet_name, data in df.items():\n",
        "    for index, row in data.iterrows():\n",
        "        # Combine text from relevant columns\n",
        "        if sheet_name == 'A-J':\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'sub_title', 'Body Text'] if pd.notna(row[col]))\n",
        "        else:\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'Body Text'] if pd.notna(row[col]))\n",
        "\n",
        "        # Preprocess text and get tokens\n",
        "        tokens = preprocess_text(combined_text)\n",
        "        print(tokens)\n",
        "        # Extract vectors for each word\n",
        "         vectors = []\n",
        "        for word in tokens:\n",
        "            if word in glove_model:\n",
        "                vectors.append(glove_model[word])\n",
        "\n",
        "        # If there are word vectors for the document, compute the average\n",
        "        if vectors:\n",
        "            avg_vector = np.mean(vectors, axis=0)\n",
        "            results.append([sheet_name, index] + avg_vector.tolist())\n",
        "\n",
        "# Save results to a CSV file\n",
        "header = [\"Sheet\", \"RowIndex\"] + [f\"Dim{i}\" for i in range(glove_model.vector_size)]\n",
        "with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(header)\n",
        "    writer.writerows(results)\n",
        "\n",
        "print(f\"Word vectors saved to {output_file}\")\n"
      ],
      "id": "de1d94cd4058a16e"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T13:37:57.946489Z",
          "start_time": "2024-12-03T13:37:26.197784Z"
        },
        "id": "bb7867b8bcf057bf",
        "outputId": "d8c0cc2d-8ad0-4673-e682-479c103c8f2c"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# File path\n",
        "output_file = \"output_files/w2v_clean_vectors.csv\"  # Replace with your file path\n",
        "\n",
        "try:\n",
        "    # Read the first 10 rows of the CSV file\n",
        "    data = pd.read_csv(output_file)\n",
        "    print(data.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"File '{output_file}' not found. Please check the file path.\")"
      ],
      "id": "bb7867b8bcf057bf",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Sheet  RowIndex       Word      Dim0     Dim1     Dim2     Dim3     Dim4  \\\n",
            "0   A-J         0       pope -0.041353 -0.25456 -0.26952 -0.70652  0.19902   \n",
            "1   A-J         0     renews  0.061107 -0.43574 -0.22176  0.21351  0.46934   \n",
            "2   A-J         0       call  0.048021  0.13645 -0.33734  0.24853  0.34074   \n",
            "3   A-J         0       gaza  0.350890 -0.74485  0.10422  0.16339 -0.60023   \n",
            "4   A-J         0  ceasefire  1.192300  0.33878  0.36272  0.49263 -0.75752   \n",
            "\n",
            "       Dim5      Dim6  ...    Dim290   Dim291   Dim292    Dim293   Dim294  \\\n",
            "0  0.124060 -0.288270  ...  0.033295 -0.33427  0.47666  0.276180 -0.17720   \n",
            "1 -0.057211  0.009017  ... -0.520890  0.29620 -0.10997 -0.852040 -0.26484   \n",
            "2 -0.310870  0.010738  ... -0.201300 -0.66498 -0.42536  0.077626  0.34800   \n",
            "3  0.427600 -0.196340  ... -0.932260 -0.82188  0.10704  0.413850  0.33142   \n",
            "4  0.396450  0.480260  ... -0.780520 -0.33792 -0.48446 -0.538560  0.28775   \n",
            "\n",
            "    Dim295   Dim296   Dim297    Dim298    Dim299  \n",
            "0 -0.51197 -0.59490  0.14016 -0.615360 -0.123000  \n",
            "1 -0.26934 -0.27907  0.16383 -0.322090  0.036824  \n",
            "2 -0.17037 -0.18028 -0.27052 -0.129220 -0.013268  \n",
            "3  0.87830 -0.25832 -0.38900 -0.769030  0.283940  \n",
            "4  1.36670  0.29385 -0.12973  0.045015  0.383350  \n",
            "\n",
            "[5 rows x 303 columns]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T15:37:11.214636Z",
          "start_time": "2024-12-03T15:36:37.002858Z"
        },
        "id": "b5c241be9e20fb1d",
        "outputId": "3530625e-17d7-42ab-e384-f9418dd25428"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Word2Vec results into a DataFrame\n",
        "input_file = \"output_files/w2v_clean_vectors.csv\"\n",
        "output_file = \"output_files/word2vec_mean_vectors.csv\"\n",
        "\n",
        "# Load the word vectors\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "# Group by Sheet and RowIndex and compute the mean for each dimension\n",
        "dim_columns = [col for col in df.columns if col.startswith(\"Dim\")]\n",
        "doc_vectors = (\n",
        "    df.groupby([\"Sheet\", \"RowIndex\"])[dim_columns]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "doc_vectors.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Averaged document vectors saved to {output_file}\")\n"
      ],
      "id": "b5c241be9e20fb1d",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Averaged document vectors saved to output_files/word2vec_mean_vectors.csv\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "41519c2a92ef75bf"
      },
      "cell_type": "raw",
      "source": [],
      "id": "41519c2a92ef75bf"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T15:37:17.104512Z",
          "start_time": "2024-12-03T15:37:16.906229Z"
        },
        "id": "abeaa01d536f9ec2",
        "outputId": "3b697a2a-1867-4fb9-f1f3-0efe64cd3e18"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "output_file = \"output_files/word2vec_mean_vectors.csv\"  # Replace with your file path\n",
        "\n",
        "try:\n",
        "    # Read the first 10 rows of the CSV file\n",
        "    data = pd.read_csv(output_file)\n",
        "    print(data.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"File '{output_file}' not found. Please check the file path.\")"
      ],
      "id": "abeaa01d536f9ec2",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Sheet  RowIndex      Dim0      Dim1      Dim2      Dim3      Dim4      Dim5  \\\n",
            "0   A-J         0 -0.002207 -0.088804  0.013135 -0.075511 -0.040838  0.044297   \n",
            "1   A-J         1  0.094648 -0.006871 -0.039487  0.051334  0.044967  0.019326   \n",
            "2   A-J         2  0.142312  0.004286  0.119358 -0.019714 -0.060275  0.050093   \n",
            "3   A-J         3  0.003800 -0.003572  0.028039 -0.053378 -0.006091 -0.205901   \n",
            "4   A-J         4 -0.185984 -0.037074  0.066869 -0.093242 -0.048560 -0.171724   \n",
            "\n",
            "       Dim6      Dim7  ...    Dim290    Dim291    Dim292    Dim293    Dim294  \\\n",
            "0 -0.164953 -0.063671  ... -0.244645 -0.184948  0.009335 -0.022688 -0.038492   \n",
            "1 -0.197835 -0.015570  ... -0.116167 -0.195352  0.089020  0.008323  0.166662   \n",
            "2 -0.092735 -0.079893  ... -0.355593 -0.281888 -0.068786  0.055006  0.080515   \n",
            "3 -0.025491 -0.128688  ... -0.058705 -0.391982 -0.078338 -0.022778  0.211467   \n",
            "4  0.105862  0.108491  ... -0.251308 -0.340048 -0.024320  0.148964  0.050160   \n",
            "\n",
            "     Dim295    Dim296    Dim297    Dim298    Dim299  \n",
            "0  0.157727 -0.175560 -0.015469 -0.255610 -0.000894  \n",
            "1  0.237181  0.038129 -0.194213 -0.268632  0.058124  \n",
            "2  0.661838 -0.045974 -0.241922 -0.237916 -0.028039  \n",
            "3  0.283444  0.116774 -0.183320 -0.259525  0.010339  \n",
            "4  0.336456  0.110887 -0.277940 -0.331213  0.018773  \n",
            "\n",
            "[5 rows x 302 columns]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "bb8d4da48dfe42fc"
      },
      "cell_type": "markdown",
      "source": [
        "Part 4: doc2vec"
      ],
      "id": "bb8d4da48dfe42fc"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T13:34:04.461362Z",
          "start_time": "2024-12-03T13:34:04.441263Z"
        },
        "id": "da0176c45483930a"
      },
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
      ],
      "id": "da0176c45483930a",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "7f53f8cc076dd623"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "source_file = \"posts_first_targil.xlsx\"\n",
        "\n",
        "# Load source Excel file\n",
        "df = pd.read_excel(source_file, sheet_name=None)\n",
        "if \"J-P\" in df:\n",
        "    df[\"J-P\"].rename(columns={\"Body\": \"Body Text\"}, inplace=True)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r\"\\d+\", \"\", text)  # Remove digits\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
        "    return word_tokenize(text)  # Tokenize the text\n",
        "\n",
        "# Prepare TaggedDocuments\n",
        "tagged_documents = []\n",
        "for sheet_name, data in df.items():\n",
        "    for index, row in data.iterrows():\n",
        "        if sheet_name == \"A-J\":\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'sub_title', 'Body Text'] if pd.notna(row[col]))\n",
        "        else:\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'Body Text'] if pd.notna(row[col]))\n",
        "\n",
        "        tokens = preprocess_text(combined_text)\n",
        "        tagged_documents.append(TaggedDocument(words=tokens, tags=[f\"{sheet_name}_{index}\"]))\n",
        "\n",
        "# Train Doc2Vec model\n",
        "model = Doc2Vec(vector_size=300, min_count=2, epochs=40, workers=4)\n",
        "model.build_vocab(tagged_documents)\n",
        "model.train(tagged_documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "# Save document vectors to CSV\n",
        "output_file = \"output_files/doc2vec_vectors.csv\"\n",
        "header = \"Sheet,RowIndex,\" + \",\".join([f\"Dim{i}\" for i in range(model.vector_size)])\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(header + \"\\n\")\n",
        "    for doc_id, doc in enumerate(tagged_documents):\n",
        "        # Extract sheet name and row index from doc.tags[0]\n",
        "        sheet, row_index = doc.tags[0].split(\"_\")\n",
        "        vector = model.dv[doc.tags[0]].tolist()\n",
        "        file.write(f\"{sheet},{row_index},\" + \",\".join(map(str, vector)) + \"\\n\")\n",
        "\n",
        "print(f\"Document vectors with RowIndex saved to {output_file}\")"
      ],
      "id": "7f53f8cc076dd623"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T13:45:57.412346Z",
          "start_time": "2024-12-03T13:45:57.160168Z"
        },
        "id": "c21547d608da067a",
        "outputId": "2becff9b-4799-4d3d-d220-edddec836b24"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "output_file = \"output_files/doc2vec_vectors.csv\"  # Replace with your file path\n",
        "\n",
        "try:\n",
        "    # Read the first 10 rows of the CSV file\n",
        "    data = pd.read_csv(output_file)\n",
        "    print(data.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"File '{output_file}' not found. Please check the file path.\")"
      ],
      "id": "c21547d608da067a",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Sheet  RowIndex      Dim0      Dim1      Dim2      Dim3      Dim4      Dim5  \\\n",
            "0   A-J         0 -0.120381  0.379345  0.134694 -0.177616 -0.337673  0.041946   \n",
            "1   A-J         1 -0.067182  0.138951  0.090573  0.097399  0.135496 -0.669350   \n",
            "2   A-J         2  0.025135  0.409332  0.405838  0.012679 -0.301956 -0.096406   \n",
            "3   A-J         3  0.036234  0.189337  0.372502  0.030741  0.225192 -0.578521   \n",
            "4   A-J         4 -0.228523  0.438873  0.277208  0.343170 -0.130110 -0.245843   \n",
            "\n",
            "       Dim6      Dim7  ...    Dim290    Dim291    Dim292    Dim293    Dim294  \\\n",
            "0  0.782558  0.720419  ...  0.071085  0.264771  0.607232  0.047377  0.579811   \n",
            "1  0.080852  0.947449  ... -0.025642  0.406675  0.322913 -0.005030  0.661448   \n",
            "2  0.210589  0.551659  ...  0.046759  0.205108  0.021800  0.378580  0.208742   \n",
            "3  0.198960  0.444680  ... -0.048156  0.490379  0.326162 -0.107866  0.724978   \n",
            "4  0.018259  0.585959  ...  0.031302  0.473778  0.751093 -0.053052  0.747913   \n",
            "\n",
            "     Dim295    Dim296    Dim297    Dim298    Dim299  \n",
            "0  0.348717 -0.109349 -0.297170  0.090944  0.191706  \n",
            "1  0.231077  0.241409 -0.196228 -0.126254  0.397652  \n",
            "2  0.213977 -0.080312  0.004675  0.004685 -0.248062  \n",
            "3  0.847633  0.372963 -0.118219  0.098637  0.038299  \n",
            "4  0.520868  0.551713 -0.533647  0.727080 -0.115523  \n",
            "\n",
            "[5 rows x 302 columns]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "8f86736ea581373"
      },
      "cell_type": "markdown",
      "source": [
        "Part 5: BERT"
      ],
      "id": "8f86736ea581373"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T13:51:42.029918Z",
          "start_time": "2024-12-03T13:51:17.399863Z"
        },
        "id": "15b0f04eab81e73e",
        "outputId": "923eba6f-a9dc-48f2-cfb2-f197e0a88af5"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "id": "15b0f04eab81e73e",
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Daniel\\PycharmProjects\\IR-exercise\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "1a585e35a08a34b9"
      },
      "cell_type": "raw",
      "source": [],
      "id": "1a585e35a08a34b9"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T14:21:27.853815Z",
          "start_time": "2024-12-03T13:54:35.123354Z"
        },
        "id": "aab02d9a825faf65",
        "outputId": "e0da2787-db69-4d76-df96-eaac81cf89ab"
      },
      "cell_type": "code",
      "source": [
        "source_file = \"posts_first_targil.xlsx\"\n",
        "output_file = \"output_files/bert_vectors.csv\"\n",
        "\n",
        "# Load source documents\n",
        "df = pd.read_excel(source_file, sheet_name=None)\n",
        "if \"J-P\" in df:\n",
        "    df[\"J-P\"].rename(columns={\"Body\": \"Body Text\"}, inplace=True)\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    return word_tokenize(text)  # Simple tokenization for this example\n",
        "\n",
        "# Function to generate BERT vectors\n",
        "def get_bert_vector(text):\n",
        "    # Tokenize and encode\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        outputs = model(**inputs)\n",
        "    # Use the CLS token as the document vector\n",
        "    cls_embedding = outputs.last_hidden_state[0, 0, :].numpy()\n",
        "    return cls_embedding\n",
        "\n",
        "\n",
        "results = []\n",
        "for sheet_name, data in df.items():\n",
        "    for index, row in data.iterrows():\n",
        "        if sheet_name == \"A-J\":\n",
        "             combined_text = \" \".join(str(row[col]) for col in ['title', 'sub_title', 'Body Text'] if pd.notna(row[col]))\n",
        "        else:\n",
        "             combined_text = \" \".join(str(row[col]) for col in ['title', 'Body Text'] if pd.notna(row[col]))\n",
        "\n",
        "        # Preprocess and generate vector\n",
        "        bert_vector = get_bert_vector(combined_text)\n",
        "        # Add RowIndex column\n",
        "        results.append([sheet_name, index] + bert_vector.tolist())\n",
        "\n",
        "# Save vectors to CSV\n",
        "header = [\"Sheet\", \"RowIndex\"] + [f\"Dim{i}\" for i in range(bert_vector.shape[0])]\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(\",\".join(header) + \"\\n\")\n",
        "    for row in results:\n",
        "        file.write(\",\".join(map(str, row)) + \"\\n\")\n",
        "\n",
        "print(f\"BERT vectors with RowIndex saved to {output_file}\")\n"
      ],
      "id": "aab02d9a825faf65",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT vectors with RowIndex saved to bert_vectors.csv\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T14:33:01.834683Z",
          "start_time": "2024-12-03T14:33:01.486817Z"
        },
        "id": "39d308134b4f0017",
        "outputId": "8e9de5c4-1548-4e87-e9ee-33fc0cb32502"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "output_file = \"output_files/bert_vectors.csv\"  # Replace with your file path\n",
        "\n",
        "try:\n",
        "    data = pd.read_csv(output_file)\n",
        "    print(data.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"File '{output_file}' not found. Please check the file path.\")"
      ],
      "id": "39d308134b4f0017",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Sheet  RowIndex      Dim0      Dim1      Dim2      Dim3      Dim4      Dim5  \\\n",
            "0   A-J         0 -1.104568 -0.098170  0.124692 -0.288394 -0.783695  0.428429   \n",
            "1   A-J         1 -0.534721  0.003136 -0.760571 -0.101115 -0.659938  0.141459   \n",
            "2   A-J         2 -0.405472  0.057184 -0.284028 -0.544374 -0.807316 -0.168701   \n",
            "3   A-J         3 -0.529135 -0.265460 -0.528176  0.095607 -0.412520  0.068867   \n",
            "4   A-J         4 -0.370012 -0.037674 -0.243967 -0.144469 -0.041479  0.415056   \n",
            "\n",
            "       Dim6      Dim7  ...    Dim758    Dim759    Dim760    Dim761    Dim762  \\\n",
            "0  0.633064  0.763848  ...  0.404507  0.029734  0.554607 -0.370775  0.848014   \n",
            "1  0.375135  0.971600  ... -0.155197  0.185688  0.345379 -0.207762  0.251543   \n",
            "2  0.673444  1.003882  ...  0.325744  0.410743  0.708710  0.052671  0.147199   \n",
            "3  0.141250  0.351869  ...  0.651753 -0.375015  0.744730 -0.014957  0.029713   \n",
            "4  0.162453  0.236116  ...  0.242417 -0.252672  0.484341 -0.148073  0.382533   \n",
            "\n",
            "     Dim763    Dim764    Dim765    Dim766    Dim767  \n",
            "0 -0.590350  0.067127 -0.352601  0.896228 -0.238273  \n",
            "1 -0.296205 -0.351764 -0.255457  0.597832  0.374410  \n",
            "2 -0.139205  0.003947 -0.608516  0.487547 -0.288179  \n",
            "3 -0.077649  0.056514 -0.776385  0.112564  0.020050  \n",
            "4 -0.260775 -0.240819 -0.071651  0.305129  0.257564  \n",
            "\n",
            "[5 rows x 770 columns]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "3c633bc0ce03c0ce"
      },
      "cell_type": "raw",
      "source": [],
      "id": "3c633bc0ce03c0ce"
    },
    {
      "metadata": {
        "id": "de8048c6d164ba48"
      },
      "cell_type": "markdown",
      "source": [
        "Part 6: Ssentence_BERT\n"
      ],
      "id": "de8048c6d164ba48"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T14:40:15.370502Z",
          "start_time": "2024-12-03T14:40:07.456884Z"
        },
        "id": "8391c224f8bba099",
        "outputId": "bcd3fc52-4ca6-4a4c-9284-67ff47960d52"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import nltk"
      ],
      "id": "8391c224f8bba099",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\Daniel\\PycharmProjects\\IR-exercise\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T14:43:24.716355Z",
          "start_time": "2024-12-03T14:41:52.926674Z"
        },
        "id": "dd18ad691ae6613c",
        "outputId": "e1883d1e-2127-4738-f8f0-7bfdef63eeea"
      },
      "cell_type": "code",
      "source": [
        "# Download NLTK resources if needed\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# File path to source documents\n",
        "source_file = \"posts_first_targil.xlsx\"\n",
        "output_file = \"output_files/sbert_vectors.csv\"\n",
        "\n",
        "# Load source documents\n",
        "df = pd.read_excel(source_file, sheet_name=None)\n",
        "if \"J-P\" in df:\n",
        "    df[\"J-P\"].rename(columns={\"Body\": \"Body Text\"}, inplace=True)\n",
        "\n",
        "# Load pre-trained SBERT model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "results = []\n",
        "for sheet_name, data in df.items():\n",
        "    for index, row in data.iterrows():\n",
        "        if sheet_name == \"A-J\":\n",
        "             combined_text = \" \".join(str(row[col]) for col in ['title', 'sub_title', 'Body Text'] if pd.notna(row[col]))\n",
        "        else:\n",
        "             combined_text = \" \".join(str(row[col]) for col in ['title', 'Body Text'] if pd.notna(row[col]))\n",
        "        # Generate SBERT vector\n",
        "        vector = model.encode(combined_text).tolist()\n",
        "        # Append results\n",
        "        results.append([sheet_name, index] + vector)\n",
        "\n",
        "# Save results to CSV\n",
        "header = [\"Sheet\", \"RowIndex\"] + [f\"Dim{i}\" for i in range(len(results[0]) - 2)]\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(\",\".join(header) + \"\\n\")\n",
        "    for row in results:\n",
        "        file.write(\",\".join(map(str, row)) + \"\\n\")\n",
        "\n",
        "print(f\"SBERT vectors saved to {output_file}\")\n"
      ],
      "id": "dd18ad691ae6613c",
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SBERT vectors saved to output_files/sbert_vectors.csv\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T14:44:40.733160Z",
          "start_time": "2024-12-03T14:44:40.406418Z"
        },
        "id": "e074f962e4665658",
        "outputId": "3a807948-d1b2-4c0a-dfdd-b1514931ca18"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "output_file = \"output_files/bert_vectors.csv\"  # Replace with your file path\n",
        "\n",
        "try:\n",
        "    data = pd.read_csv(output_file)\n",
        "    print(data.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"File '{output_file}' not found. Please check the file path.\")"
      ],
      "id": "e074f962e4665658",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Sheet  RowIndex      Dim0      Dim1      Dim2      Dim3      Dim4      Dim5  \\\n",
            "0   A-J         0 -1.104568 -0.098170  0.124692 -0.288394 -0.783695  0.428429   \n",
            "1   A-J         1 -0.534721  0.003136 -0.760571 -0.101115 -0.659938  0.141459   \n",
            "2   A-J         2 -0.405472  0.057184 -0.284028 -0.544374 -0.807316 -0.168701   \n",
            "3   A-J         3 -0.529135 -0.265460 -0.528176  0.095607 -0.412520  0.068867   \n",
            "4   A-J         4 -0.370012 -0.037674 -0.243967 -0.144469 -0.041479  0.415056   \n",
            "\n",
            "       Dim6      Dim7  ...    Dim758    Dim759    Dim760    Dim761    Dim762  \\\n",
            "0  0.633064  0.763848  ...  0.404507  0.029734  0.554607 -0.370775  0.848014   \n",
            "1  0.375135  0.971600  ... -0.155197  0.185688  0.345379 -0.207762  0.251543   \n",
            "2  0.673444  1.003882  ...  0.325744  0.410743  0.708710  0.052671  0.147199   \n",
            "3  0.141250  0.351869  ...  0.651753 -0.375015  0.744730 -0.014957  0.029713   \n",
            "4  0.162453  0.236116  ...  0.242417 -0.252672  0.484341 -0.148073  0.382533   \n",
            "\n",
            "     Dim763    Dim764    Dim765    Dim766    Dim767  \n",
            "0 -0.590350  0.067127 -0.352601  0.896228 -0.238273  \n",
            "1 -0.296205 -0.351764 -0.255457  0.597832  0.374410  \n",
            "2 -0.139205  0.003947 -0.608516  0.487547 -0.288179  \n",
            "3 -0.077649  0.056514 -0.776385  0.112564  0.020050  \n",
            "4 -0.260775 -0.240819 -0.071651  0.305129  0.257564  \n",
            "\n",
            "[5 rows x 770 columns]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "b2e9a68ed03b304e"
      },
      "cell_type": "markdown",
      "source": [],
      "id": "b2e9a68ed03b304e"
    },
    {
      "metadata": {
        "id": "6f2c2aec354aa4ba"
      },
      "cell_type": "markdown",
      "source": [
        "New Word2Vec"
      ],
      "id": "6f2c2aec354aa4ba"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-10T08:56:51.486049Z",
          "start_time": "2024-12-10T08:56:46.884106Z"
        },
        "id": "e12d7ff1898f531",
        "outputId": "2a7dd786-000b-4248-8bdc-f9cc9153265e"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import gensim.downloader as api\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "import csv\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "id": "e12d7ff1898f531",
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "83a20a6966bdb442"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "input_file = \"output_files/lemma_file.xlsx\"\n",
        "output_file = \"output_files/new_w2v_lemma_vectors_2.csv\"\n",
        "\n",
        "df = pd.read_excel(input_file, sheet_name=None)\n",
        "\n",
        "if \"J-P\" in df:\n",
        "    df[\"J-P\"].rename(columns={\"Body\": \"Body Text\"}, inplace=True)\n",
        "\n",
        "try:\n",
        "    glove_model = api.load(\"glove-wiki-gigaword-300\")  # 300-dimensional GloVe vectors\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\d+\", \"\", text)  # Remove digits and dates\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
        "    tokens = word_tokenize(text)  # Tokenize the text\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    return [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
        "\n",
        "def calculate_idf(corpus):\n",
        "    vectorizer = TfidfVectorizer(use_idf=True, stop_words=\"english\")\n",
        "    vectorizer.fit(corpus)\n",
        "    idf_dict = defaultdict(lambda: 0)\n",
        "    for word, idf in zip(vectorizer.get_feature_names_out(), vectorizer.idf_):\n",
        "        idf_dict[word] = idf\n",
        "    return idf_dict\n",
        "\n",
        "corpus = []\n",
        "for sheet_name, data in df.items():\n",
        "    for index, row in data.iterrows():\n",
        "        if sheet_name == 'A-J':\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'sub_title', 'Body Text'] if pd.notna(row[col]))\n",
        "        else:\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'Body Text'] if pd.notna(row[col]))\n",
        "        corpus.append(combined_text)\n",
        "\n",
        "idf_dict = calculate_idf(corpus)\n",
        "\n",
        "\n",
        "results = []\n",
        "for sheet_name, data in df.items():\n",
        "    for index, row in data.iterrows():\n",
        "        # Combine text from relevant columns\n",
        "        if sheet_name == 'A-J':\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'sub_title', 'Body Text'] if pd.notna(row[col]))\n",
        "        else:\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'Body Text'] if pd.notna(row[col]))\n",
        "\n",
        "        # Preprocess text and get tokens\n",
        "        tokens = preprocess_text(combined_text)\n",
        "\n",
        "        # Extract vectors for each word\n",
        "        for word in tokens:\n",
        "            if word in glove_model:\n",
        "                vector = glove_model[word]\n",
        "                idf_value = idf_dict[word]\n",
        "                weighted_vector = vector * idf_value  # Multiply word vector by IDF\n",
        "                results.append([sheet_name, index, word] + weighted_vector.tolist())\n",
        "\n",
        "\n",
        "# Save results to a CSV file\n",
        "header = [\"Sheet\", \"RowIndex\", \"Word\"] + [f\"Dim{i}\" for i in range(glove_model.vector_size)]\n",
        "with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(header)\n",
        "    writer.writerows(results)\n",
        "\n",
        "print(f\"Word vectors saved to {output_file}\")\n",
        "\n",
        "\n",
        "output_file = \"output_files/new_w2v_lemma_vectors_2.csv\"\n",
        "\n",
        "try:\n",
        "    data = pd.read_csv(output_file, nrows=10)\n",
        "    print(data)\n",
        "except FileNotFoundError:\n",
        "    print(f\"File '{output_file}' not found. Please check the file path.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Load the Word2Vec results into a DataFrame\n",
        "input_file = \"output_files/new_w2v_lemma_vectors_2.csv\"\n",
        "output_file = \"output_files/new_word2vec_mean_vectors.csv\"\n",
        "\n",
        "# Load the word vectors\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "# Group by Sheet and RowIndex and compute the mean for each dimension\n",
        "dim_columns = [col for col in df.columns if col.startswith(\"Dim\")]\n",
        "doc_vectors = (\n",
        "    df.groupby([\"Sheet\", \"RowIndex\"])[dim_columns]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "doc_vectors.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Averaged document vectors saved to {output_file}\")"
      ],
      "id": "83a20a6966bdb442"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-10T08:56:53.252993Z",
          "start_time": "2024-12-10T08:56:53.084198Z"
        },
        "id": "b7cf033c59acb626",
        "outputId": "fdfbc99e-27be-4ce3-f8e3-c71a025b51e9"
      },
      "cell_type": "code",
      "source": [
        "output_file = \"output_files/new_word2vec_mean_vectors.csv\"\n",
        "\n",
        "try:\n",
        "    data = pd.read_csv(output_file)\n",
        "    print(data.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"File '{output_file}' not found. Please check the file path.\")"
      ],
      "id": "b7cf033c59acb626",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Sheet  RowIndex      Dim0      Dim1      Dim2      Dim3      Dim4      Dim5  \\\n",
            "0   A-J         0 -0.000125 -0.333324 -0.341119 -0.585946  0.073495  0.275870   \n",
            "1   A-J         1  0.361846 -0.050101 -0.036313  0.231563  0.349663  0.204377   \n",
            "2   A-J         2  0.318587  0.066093  0.150641  0.077817 -0.115447  0.012682   \n",
            "3   A-J         3  0.048476  0.098860 -0.045120 -0.251414  0.151417 -0.714273   \n",
            "4   A-J         4 -0.546905 -0.067917  0.356877 -0.676202 -0.028615 -0.581645   \n",
            "\n",
            "       Dim6      Dim7  ...    Dim290    Dim291    Dim292    Dim293    Dim294  \\\n",
            "0 -0.677872 -0.406010  ... -0.521829 -0.597251  0.289087  0.014901 -0.410971   \n",
            "1 -0.375672 -0.355330  ... -0.123543 -0.379267  0.451864  0.115989  0.448655   \n",
            "2 -0.127426 -0.112411  ... -0.915440 -0.545861 -0.277455 -0.059635  0.272248   \n",
            "3 -0.400813 -0.723303  ... -0.049962 -1.223554 -0.251819 -0.093594  0.839417   \n",
            "4  0.192781  0.093955  ... -0.704663 -0.861301  0.056560  0.265106  0.278741   \n",
            "\n",
            "     Dim295    Dim296    Dim297    Dim298    Dim299  \n",
            "0  0.088925 -0.653406 -0.123957 -0.873498  0.035305  \n",
            "1  0.181676  0.184972 -0.482779 -0.705563  0.270827  \n",
            "2  1.373893 -0.228123 -0.628487 -0.580959  0.106218  \n",
            "3  1.086166  0.478767 -0.387084 -1.124998  0.186913  \n",
            "4  0.784440  0.388029 -0.816524 -1.057894  0.216149  \n",
            "\n",
            "[5 rows x 302 columns]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "5ec99fd5f5dabfaf"
      },
      "cell_type": "markdown",
      "source": [],
      "id": "5ec99fd5f5dabfaf"
    },
    {
      "metadata": {
        "id": "3799357dd521d293"
      },
      "cell_type": "markdown",
      "source": [
        "New BERT"
      ],
      "id": "3799357dd521d293"
    },
    {
      "metadata": {
        "id": "3e0cbb911dd3cb07"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n"
      ],
      "id": "3e0cbb911dd3cb07"
    },
    {
      "metadata": {
        "id": "41c3ed487e70439a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "source_file = \"posts_first_targil.xlsx\"\n",
        "output_file = \"output_files/new_bert_vectors.csv\"\n",
        "\n",
        "df = pd.read_excel(source_file, sheet_name=None)\n",
        "if \"J-P\" in df:\n",
        "    df[\"J-P\"].rename(columns={\"Body\": \"Body Text\"}, inplace=True)\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "def calculate_idf(corpus):\n",
        "    vectorizer = TfidfVectorizer(use_idf=True, stop_words=\"english\")\n",
        "    vectorizer.fit(corpus)\n",
        "    idf_dict = defaultdict(lambda: 0)\n",
        "    for word, idf in zip(vectorizer.get_feature_names_out(), vectorizer.idf_):\n",
        "        idf_dict[word] = idf\n",
        "    return idf_dict\n",
        "\n",
        "corpus = []\n",
        "for sheet_name, data in df.items():\n",
        "    for index, row in data.iterrows():\n",
        "        if sheet_name == 'A-J':\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'sub_title', 'Body Text'] if pd.notna(row[col]))\n",
        "        else:\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'Body Text'] if pd.notna(row[col]))\n",
        "        corpus.append(combined_text)\n",
        "\n",
        "idf_dict = calculate_idf(corpus)\n",
        "\n",
        "\n",
        "def get_bert_vectors(text_chunk):\n",
        "    inputs = tokenizer(text_chunk, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    token_embeddings = outputs.last_hidden_state.squeeze(0)  # Shape: [sequence_length, hidden_size]\n",
        "    attention_mask = inputs[\"attention_mask\"].squeeze(0)  # Shape: [sequence_length]\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze(0))  # List of tokens\n",
        "\n",
        "    return tokens, token_embeddings, attention_mask\n",
        "\n",
        "# Function to process subwords into full word embeddings\n",
        "def process_tokens(tokens, token_embeddings, attention_mask, idf_dict):\n",
        "    word_embeddings = []\n",
        "    current_word = \"\"\n",
        "    current_word_vectors = []\n",
        "\n",
        "    for token, embedding, mask in zip(tokens, token_embeddings, attention_mask):\n",
        "        if mask == 0 or token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
        "            continue\n",
        "\n",
        "        if token.startswith(\"##\"):  # Subword continuation\n",
        "            current_word += token[2:]\n",
        "            current_word_vectors.append(embedding)\n",
        "        else:  # New word starts\n",
        "            if current_word:  # Combine previous word embeddings\n",
        "                combined_embedding = torch.mean(torch.stack(current_word_vectors), dim=0)\n",
        "                idf = idf_dict.get(current_word, 1.0)  # Default IDF to 1.0 if not found\n",
        "                word_embeddings.append(combined_embedding * idf)\n",
        "\n",
        "            # Start new word\n",
        "            current_word = token\n",
        "            current_word_vectors = [embedding]\n",
        "\n",
        "    # Process the last word\n",
        "    if current_word:\n",
        "        combined_embedding = torch.mean(torch.stack(current_word_vectors), dim=0)\n",
        "        idf = idf_dict.get(current_word, 1.0)\n",
        "        word_embeddings.append(combined_embedding * idf)\n",
        "\n",
        "    return word_embeddings\n",
        "# Function to process an entire document\n",
        "def process_document(text, idf_dict):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    max_tokens = 512\n",
        "    num_chunks = (len(tokens) + max_tokens - 1) // max_tokens  # Ceiling division\n",
        "    all_word_embeddings = []\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        chunk_tokens = tokens[i * max_tokens : (i + 1) * max_tokens]\n",
        "        chunk_text = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
        "        tokens, embeddings, attention_mask = get_bert_vectors(chunk_text)\n",
        "        chunk_word_embeddings = process_tokens(tokens, embeddings, attention_mask, idf_dict)\n",
        "        all_word_embeddings.extend(chunk_word_embeddings)\n",
        "\n",
        "    # Aggregate all word embeddings for the document (e.g., by mean or sum)\n",
        "    document_embedding = torch.mean(torch.stack(all_word_embeddings), dim=0)\n",
        "    return document_embedding\n",
        "\n",
        "\n",
        "results = []\n",
        "for sheet_name, data in df.items():\n",
        "    for index, row in data.iterrows():\n",
        "        if sheet_name == \"A-J\":\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'sub_title', 'Body Text'] if pd.notna(row[col]))\n",
        "        else:\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'Body Text'] if pd.notna(row[col]))\n",
        "\n",
        "        # Generate BERT vectors for the document\n",
        "        bert_vector = process_document(combined_text, idf_dict)\n",
        "        vector_list = bert_vector.tolist()\n",
        "        results.append([sheet_name, index] + vector_list)\n",
        "        print(vector_list)\n",
        "\n",
        "\n",
        "\n",
        "# Save vectors to CSV\n",
        "header = [\"Sheet\", \"RowIndex\"] + [f\"Dim{i}\" for i in range(bert_vector.shape[0])]\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(\",\".join(header) + \"\\n\")\n",
        "    for row in results:\n",
        "        file.write(\",\".join(map(str, row)) + \"\\n\")\n",
        "\n",
        "print(f\"BERT vectors with RowIndex saved to {output_file}\")"
      ],
      "id": "41c3ed487e70439a"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-10T08:58:08.898478Z",
          "start_time": "2024-12-10T08:58:08.544064Z"
        },
        "id": "d45b78118b6403e0",
        "outputId": "925682c5-b56c-45d0-be1e-fdc1c4208528"
      },
      "cell_type": "code",
      "source": [
        "output_file = \"output_files/new_bert_vectors.csv\"\n",
        "\n",
        "try:\n",
        "    data = pd.read_csv(output_file)\n",
        "    print(data.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"File '{output_file}' not found. Please check the file path.\")"
      ],
      "id": "d45b78118b6403e0",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Sheet  RowIndex      Dim0      Dim1      Dim2      Dim3      Dim4      Dim5  \\\n",
            "0   A-J         0 -0.788050  0.056614  1.110174 -1.022818  0.147041 -0.474798   \n",
            "1   A-J         1 -0.003478  0.293304 -1.017949 -0.549554  0.197699 -0.491028   \n",
            "2   A-J         2 -0.168909  0.331912  0.046262 -0.966383 -0.039441 -1.237246   \n",
            "3   A-J         3 -0.188200 -0.066150  0.061154 -0.881887  0.318995 -0.174752   \n",
            "4   A-J         4  0.367581  0.150649 -0.167966 -0.829529  0.578362 -0.619802   \n",
            "\n",
            "       Dim6      Dim7  ...    Dim758    Dim759    Dim760    Dim761    Dim762  \\\n",
            "0  0.684777  1.736403  ...  0.441260 -0.295122  1.369656 -0.888012  1.068354   \n",
            "1 -0.230678  0.669581  ...  0.126593  0.276906  0.418110 -0.073043  0.532075   \n",
            "2  0.228654  1.053331  ...  0.301428  0.401214  0.752306 -0.367575  0.183579   \n",
            "3 -0.718530  1.247533  ...  0.775314  0.303543  0.816760 -0.345624 -0.053506   \n",
            "4 -0.128312  0.784007  ...  0.429937  0.090282  0.748448 -0.224150  0.255825   \n",
            "\n",
            "     Dim763    Dim764    Dim765    Dim766    Dim767  \n",
            "0 -1.006198 -0.373084 -0.121401  0.322801  0.021724  \n",
            "1 -1.112056  0.301055 -0.610574 -0.392991  0.147567  \n",
            "2 -1.709214 -0.223334 -1.435423 -0.652594 -0.533352  \n",
            "3 -1.028428 -0.775664 -1.202621 -0.520046  0.887129  \n",
            "4 -1.365864 -0.680425 -1.341126  0.401274  0.005645  \n",
            "\n",
            "[5 rows x 770 columns]\n"
          ]
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}